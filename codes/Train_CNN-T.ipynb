{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import *\n",
    "from numpy import dot, multiply, diag, power\n",
    "from numpy import pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv,norm\n",
    "from scipy.linalg import svd, svdvals\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from functools import partial\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from __future__ import print_function\n",
    "from sklearn import preprocessing  \n",
    "from sklearn import metrics \n",
    "import scipy.io as sio \n",
    "import time  \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "model_name ='modelC' \n",
    "T=120; time_step=1/30; \n",
    "thres=0.9995 ;gap=0; dimx=9 \n",
    "patience = 12\n",
    "n_classes=3\n",
    "layers=3 \n",
    "t01=122;t02=t01+T \n",
    "num_datacase=6\n",
    "pred_thres=0.9999 \n",
    "pred_gap=0\n",
    "lambda_loss_amount = 0.001 \n",
    "batch_norm=1   \n",
    "learning_rate = 0.001\n",
    "training_iters =5000\n",
    "batch_size =50\n",
    "display_step = 300\n",
    "keep_prob=1\n",
    "dropout=0 \n",
    "savepath = '/codes/saved_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "rootpath = '/data/train/time_series_data'\n",
    "train_data  = np.load(os.path.join(rootpath, 'train_data.npy'   ) )\n",
    "train_labels =np.load(os.path.join(rootpath, 'train_labels.npy' ) ) \n",
    "eval_data=np.load(os.path.join(rootpath, 'eval_data.npy'  ) )\n",
    "eval_labels=np.load(os.path.join(rootpath, 'eval_labels.npy' ) ) \n",
    "testpath ='/data/test/time_series_data/total' \n",
    "testX_pred =np.load(os.path.join(testpath, 'testX_pred.npy' ) )\n",
    "testY_pred=np.load(os.path.join(testpath, 'testY_pred.npy'  ) ) \n",
    "testX_no=np.load(os.path.join(testpath, 'testX_no.npy' ) )\n",
    "testY_no=np.load(os.path.join(testpath, 'testY_no.npy' ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, stride_row,stride_col):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride_row,stride_col, 1], padding='VALID') \n",
    "    x = tf.nn.bias_add(x, b)  \n",
    "    return tf.nn.relu(x)  \n",
    "\n",
    "def batch_norm(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('bn'):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n",
    "def conv2d_norm(x,W,b, phase_train,stride_row,stride_col):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride_row,stride_col, 1], padding='VALID')  \n",
    "    x = tf.nn.bias_add(x, b)  \n",
    "    x_out = batch_norm(x,1,phase_train)\n",
    "    return tf.nn.relu(x_out ) \n",
    "\n",
    "def maxpool2d(x, height,width):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, height,  width, 1], strides=[1,  height,width, 1],\n",
    "                          padding='SAME') # \n",
    "def input_weight_all(Theta,name):# Theta is a list type\n",
    "    import pickle\n",
    "    filepointer=open(name,\"wb\")\n",
    "    pickle.dump(Theta,filepointer,protocol=2)\n",
    "    filepointer.close()\n",
    "    return \n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var) \n",
    "# Create model\n",
    "def conv_net( x,   batch_norm,phase_train):  \n",
    "    global layers, dimx, keep_prob  \n",
    "    dep1=8;dep2=16;dep3=16; \n",
    "    x = tf.reshape(x, shape=[-1,214,30,1])  \n",
    "    with tf.variable_scope('eigen_conv1'): \n",
    "        wc1= tf.get_variable( 'weight1',shape = [5,5,1,dep1],\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)) \n",
    "        bc1=tf.get_variable( 'bias1',\n",
    "          shape = [dep1],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        stride_row=2;stride_col=2\n",
    "        if batch_norm:\n",
    "            conv1 = conv2d_norm(x,wc1,bc1,phase_train,stride_row,stride_col) \n",
    "            conv1 = maxpool2d(conv1,2,2)    \n",
    "        else:\n",
    "            conv1 = conv2d(x, wc1, bc1,stride_row,stride_col)  \n",
    "            conv1 = maxpool2d(conv1,2,2 )  \n",
    "        variable_summaries(wc1)\n",
    "        variable_summaries(bc1)  \n",
    "    with tf.variable_scope('eigen_conv2'): \n",
    "        wc2= tf.get_variable( 'weight2',shape = [3,3, dep1, dep2],\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)) \n",
    "        bc2=tf.get_variable( 'bias2',\n",
    "          shape = [dep2],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        stride_row=2;stride_col=2\n",
    "        if batch_norm:\n",
    "            conv2 = conv2d_norm(conv1,wc2,bc2, phase_train,stride_row,stride_col)\n",
    "            conv2 = maxpool2d(conv2, 2,2)   \n",
    "        else:\n",
    "            conv2 = conv2d(conv1, wc2, bc2,stride_row,stride_col) \n",
    "            conv2 = maxpool2d(conv2, 2,2) \n",
    "        variable_summaries(wc2)\n",
    "        variable_summaries(bc2)      \n",
    "    with tf.variable_scope('eigen_conv3'): \n",
    "        wc3= tf.get_variable( 'weight3',shape = [2,2, dep2, dep3],\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bc3=tf.get_variable( 'bias3',\n",
    "          shape = [dep3],\n",
    "          initializer=tf.constant_initializer(0.0))\n",
    "        stride_row=2;stride_col=2\n",
    "        if batch_norm:\n",
    "            conv3 = conv2d_norm(conv2,wc3,bc3, phase_train,stride_row,stride_col )\n",
    "            conv3 = maxpool2d(conv3, 2,1)  \n",
    "        else:\n",
    "            conv3 = conv2d(conv2, wc3, bc3,stride_row,stride_col) \n",
    "            conv3 = maxpool2d(conv3,2,1)  \n",
    "        variable_summaries(wc3)\n",
    "        variable_summaries(bc3)     \n",
    "    with tf.variable_scope('eigen_out'):\n",
    "        wfc = tf.get_variable('wfc',shape=[13*2 *dep3   , dep3],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bfc=tf.get_variable( 'bfc',\n",
    "          shape = [ dep3],\n",
    "          initializer=tf.constant_initializer(0.0)),  \n",
    "        # fully connected layer\n",
    "        fc0 = tf.reshape(conv2, [-1,  int(prod(conv2.get_shape()[1:])) ]) \n",
    "        fc1= tf.nn.relu(tf.add(tf.matmul(fc0, wfc), bfc))   \n",
    "        variable_summaries(wfc)\n",
    "        variable_summaries(bfc) \n",
    "    \n",
    "    with tf.variable_scope('Final_out'):\n",
    "        wout= tf.get_variable('wout',shape=[ dep3, n_classes],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bout=tf.get_variable( 'bout',shape = [n_classes],initializer=tf.constant_initializer(0.0)) \n",
    "        # fully connected layer \n",
    "        fc2=tf.cond( phase_train, lambda: tf.nn.dropout(fc1,keep_prob=keep_prob if dropout else 1.0),lambda:fc1 ) \n",
    "        fc2=tf.nn.relu(tf.add(tf.matmul(fc2, wout), bout))  \n",
    "    return fc2\n",
    "\n",
    "def establish_model(): \n",
    "    global keep_prob,   learning_rate, training_iters,display_step,batch_size,layers,patience \n",
    "    global train_data,train_labels,eval_data,eval_labels,testX_true,testY_true , testX_pred,testY_pred , testX_no,testY_no \n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 214,30,1])\n",
    "    y = tf.placeholder(tf.int32, [None, n_classes])  \n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "        # Construct model\n",
    "    global_step = tf.Variable(0, trainable=False)   \n",
    "    pred = conv_net(  x, norm,phase_train) \n",
    "    prob = tf.nn.softmax(pred)\n",
    "    predict_op=tf.argmax(tf.nn.softmax(pred), 1) \n",
    "    step=1\n",
    "    loss_list=[]\n",
    "    train_rate=[]\n",
    "    eval_rate=[]\n",
    "    n_incr_num =0\n",
    "    best_loss = np.Inf\n",
    "    # Define loss and optimizer \n",
    "    with tf.name_scope('loss'): \n",
    "        l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() )  \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))+l2  \n",
    "    with tf.name_scope('Optimizer'):  \n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate , decay=0.9).minimize(cost, global_step=global_step) \n",
    "    with tf.name_scope('err'): \n",
    "        correct = tf.equal(predict_op, tf.argmax(y, 1))\n",
    "        err=1- tf.reduce_mean(tf.cast(correct, tf.float32))  \n",
    "    tf.summary.scalar('err',err)\n",
    "    tf.summary.scalar('loss',cost)\n",
    "    \n",
    "    #save\n",
    "    saver = tf.train.Saver() \n",
    "    sess = tf.InteractiveSession() \n",
    "     \n",
    "    # Initializing the variables \n",
    "    init = tf.global_variables_initializer();sess.run(init)\n",
    "    if train_data.shape[1]!= 214:\n",
    "        train_data = np.transpose(train_data, [0,2, 1]) \n",
    "        train_data = np.expand_dims(train_data, 3)\n",
    "    if eval_data.shape[1]!= 214:\n",
    "        eval_data = np.transpose(eval_data,[0,2,1])\n",
    "        eval_data = np.expand_dims(eval_data,3) \n",
    "    if testX_pred.shape[1]!= 214:\n",
    "        testX_pred = np.transpose(testX_pred,[0,2,1])\n",
    "        testX_pred = np.expand_dims(testX_pred,3)\n",
    "    if testX_no.shape[1]!= 214:\n",
    "        testX_no = np.transpose(testX_no,[0,2,1])\n",
    "        testX_no = np.expand_dims(testX_no,3)\n",
    "    while step < training_iters: \n",
    "        ind = np.arange(train_data.shape[0])\n",
    "        batch_idx = np.random.choice(ind, batch_size, replace=False)\n",
    "        batch_x = train_data[batch_idx] \n",
    "        batch_y= train_labels[batch_idx] \n",
    "        indeval = np.arange(eval_data.shape[0]) \n",
    "        eval_idx = np.random.choice(indeval, batch_size, replace=False)\n",
    "        batch_xeval=eval_data[eval_idx] \n",
    "        batch_yeval=eval_labels[eval_idx]  \n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,phase_train:True }) \n",
    "        loss, train_err = sess.run([cost, err], feed_dict={x: batch_x, y: batch_y ,phase_train:True})\n",
    "        loss_eval,eval_err=sess.run([cost ,err ], feed_dict={x: batch_xeval,  y: batch_yeval ,phase_train:True})\n",
    "        loss_list.append(loss)\n",
    "        train_rate.append(train_err)\n",
    "        eval_rate.append(eval_err)\n",
    "        step += 1\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and err \n",
    "            print(\"Iter \" + str(step ) + \", Minibatch Loss= \" +  \"{:.2f}\".format(loss) + \",training err= \" + \"{:.2f}\".format(train_err)+ \",validating err= \" + \"{:.2f}\".format(eval_err))   \n",
    "            if loss_eval < best_loss:\n",
    "                best_loss = loss_eval\n",
    "                n_incr_num =0\n",
    "            else:\n",
    "                n_incr_num+=1\n",
    "            if (n_incr_num >= patience) and (step > 3000):\n",
    "                print ('Early_stopping! and the iterations is', step)\n",
    "                # Create the collection.\n",
    "                tf.get_collection(\"validation_nodes\")\n",
    "                # Add stuff to the collection.\n",
    "                tf.add_to_collection(\"validation_nodes\", x)  \n",
    "                tf.add_to_collection(\"validation_nodes\", y) \n",
    "                tf.add_to_collection(\"validation_nodes\", phase_train) \n",
    "                tf.add_to_collection(\"validation_nodes\", prob)  \n",
    "                save_path = saver.save(sess, savepath+model_name) \n",
    "                correct_results_pred=sess.run(correct , feed_dict={x:testX_pred,  y:testY_pred,phase_train:True}) \n",
    "                total_test_pred=sess.run(err , feed_dict={x:testX_pred,  y:testY_pred,phase_train:True}) \n",
    "                correct_results_no=sess.run(correct , feed_dict={x:testX_no,   y:testY_no,phase_train:True}) \n",
    "                total_test_no=sess.run(err , feed_dict={x:testX_no,  y:testY_no,phase_train:True})    \n",
    "                print(\"Testing err of subtract predicted :\", total_test_pred) \n",
    "                print(\"Testing err of subtract nothing :\", total_test_no)  \n",
    "                return loss_list,step,train_rate,eval_rate, correct_results_pred,correct_results_no \n",
    "    # Create the collection.\n",
    "    tf.get_collection(\"validation_nodes\")\n",
    "    # Add stuff to the collection.\n",
    "    tf.add_to_collection(\"validation_nodes\", x) \n",
    "    tf.add_to_collection(\"validation_nodes\", y) \n",
    "    tf.add_to_collection(\"validation_nodes\", phase_train) \n",
    "    tf.add_to_collection(\"validation_nodes\", prob) \n",
    "    save_path = saver.save(sess, savepath+model_name) \n",
    "    correct_results_pred=sess.run(correct , feed_dict={x:testX_pred,   y:testY_pred,phase_train:True}) \n",
    "    total_test_pred=sess.run(err , feed_dict={x:testX_pred,   y:testY_pred,phase_train:True}) \n",
    "    correct_results_no=sess.run(correct , feed_dict={x:testX_no,  y:testY_no,phase_train:True}) \n",
    "    total_test_no=sess.run(err , feed_dict={x:testX_no,   y:testY_no,phase_train:True}) \n",
    "    print(\"Testing err of subtract predicted :\", total_test_pred) \n",
    "    print(\"Testing err of subtract nothing :\", total_test_no)  \n",
    "    print(\"Optimization Finished!\") \n",
    "    return loss_list,step,train_rate,eval_rate,  correct_results_pred,correct_results_no  \n",
    "\n",
    "def each_perform(correct_results,eval_labels ):\n",
    "    label_y=np.argmax(eval_labels,1)\n",
    "    \n",
    "    zero = np.where(label_y==0)\n",
    "    correct=[correct_results[i] for i in zero]\n",
    "    accuracy_zero=1-np.mean(correct)\n",
    "\n",
    "    one = np.where(label_y==1)\n",
    "    correct=[correct_results[i] for i in one]\n",
    "    accuracy_one=1-np.mean(correct)\n",
    "\n",
    "    two = np.where(label_y==2)\n",
    "    correct=[correct_results[i] for i in two]\n",
    "    accuracy_two=1-np.mean(correct) \n",
    "    \n",
    "   \n",
    "    print(100-100*accuracy_zero)\n",
    "    print(100-100*accuracy_one)\n",
    "    print(100-100*accuracy_two)  \n",
    "\n",
    "def plot_loss(loss,train_step,from_second,name_save, plot_name,plot_title):\n",
    "    if from_second :\n",
    "        plt.plot(range(0,train_step-1,1),loss[1:])\n",
    "    else:\n",
    "        plt.plot(range(0,train_step,1),loss[0:])\n",
    "    plt.xlabel('Iterative times (t)')\n",
    "    plt.ylabel(plot_name)\n",
    "    plt.title(plot_title)\n",
    "    plt.grid(True) \n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=1\n",
    "if train==1:  \n",
    "    start_time = time.time()\n",
    "    loss ,step,train_rate,eval_rate,  correct_results_pred,correct_results_no=establish_model()    \n",
    "    eclapse = time.time() -start_time\n",
    "    print ('Running time is',eclapse)\n",
    "    plot_loss(loss,step-1,False,\"Loss_value.png\", 'Loss','Loss function value with iterations') \n",
    "    plot_loss(train_rate,step-1,False,\"Train_err_rate.png\",'Training accurate rate','Training classification with iterations')\n",
    "    plot_loss(eval_rate,step-1,False,\"Eval_err_rate.png\",'Evaluating accurate rate','Evaluating classification with iterations')  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
