{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import *\n",
    "from numpy import dot, multiply, diag, power\n",
    "from numpy import pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv,norm\n",
    "from scipy.linalg import svd, svdvals\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from functools import partial\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from __future__ import print_function\n",
    "from sklearn import preprocessing  \n",
    "from sklearn import metrics \n",
    "import scipy.io as sio \n",
    "import time  \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "model_name ='modelA1'\n",  
    "patience = 12\n",
    "n_classes=3\n",
    "layers=3\n",
    "s_layers=2\n",  
    "lambda_loss_amount = 0.001\n",
    "batch_norm=1   \n",
    "learning_rate = 0.001\n",
    "training_iters =9000\n",
    "batch_size =30\n",
    "display_step = 100\n",
    "keep_prob=0.9\n",
    "dropout=0  \n",
    "decay_c =0.8\n",
    "savepath =  '../codes/saved_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = '../data/train/sub_features'\n",
    "train_data  = np.load(os.path.join(rootpath, 'train_data.npy'   ) )\n",
    "train_labels =np.load(os.path.join(rootpath, 'train_labels.npy' ) )\n",
    "train_e=np.load(os.path.join(rootpath, 'train_e.npy' ) )\n",
    "eval_data=np.load(os.path.join(rootpath, 'eval_data.npy'  ) )\n",
    "eval_labels=np.load(os.path.join(rootpath, 'eval_labels.npy' ) )\n",
    "eval_e=np.load(os.path.join(rootpath, 'eval_e.npy' ))         \n",
    "testpath ='../data/test/sub_features/total' \n",
    "testX_pred =np.load(os.path.join(testpath, 'testX_pred.npy' ) )\n",
    "testY_pred=np.load(os.path.join(testpath, 'testY_pred.npy'  ) )\n",
    "Energy_pred=np.load(os.path.join(testpath, 'Energy_pred.npy' ))  \n",
    "testX_no=np.load(os.path.join(testpath, 'testX_no.npy' ) )\n",
    "testY_no=np.load(os.path.join(testpath, 'testY_no.npy' ) )\n",
    "Energy_no=np.load(os.path.join(testpath, 'Energy_no.npy'  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, stride_row,stride_col):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride_row,stride_col, 1], padding='VALID') \n",
    "    x = tf.nn.bias_add(x, b)  \n",
    "    return tf.nn.relu(x) # \n",
    "\n",
    "def batch_norm(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('bn'):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n",
    "def conv2d_norm(x,W,b, phase_train,stride_row,stride_col):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride_row,stride_col, 1], padding='VALID')  \n",
    "    x = tf.nn.bias_add(x, b)  \n",
    "    x_out = batch_norm(x,1,phase_train)\n",
    "    return tf.nn.relu(x_out ) \n",
    "\n",
    "def maxpool2d(x, height,width): \n",
    "    return tf.nn.max_pool(x, ksize=[1, height,  width, 1], strides=[1,  height,width, 1],\n",
    "                          padding='VALID')  \n",
    "\n",
    "def input_weight_all(Theta,name): \n",
    "    import pickle\n",
    "    filepointer=open(name,\"wb\")\n",
    "    pickle.dump(Theta,filepointer,protocol=2)\n",
    "    filepointer.close()\n",
    "    return \n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var) \n",
    "            \n",
    "def conv_net( x, Energy, batch_norm,phase_train):  \n",
    "    global layers, dimx, keep_prob \n",
    "    sample_num = np.shape(x)[0]\n",
    "    dep1=4;dep2=8; dep3=16; \n",
    "    x = tf.reshape(x, shape=[-1,dimx,layers,1]) \n",
    "    Energy = tf.reshape(Energy, shape=[-1, dimx,s_layers,1]) \n",
    "    with tf.variable_scope('eigen_conv1'): \n",
    "        wc1= tf.get_variable( 'weight1',shape = [2,1, 1, dep1],\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)) \n",
    "        bc1=tf.get_variable( 'bias1',\n",
    "          shape = [dep1],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        stride_row=1;stride_col=1\n",
    "        if batch_norm:\n",
    "            conv1 = conv2d_norm(x,wc1,bc1,phase_train,stride_row,stride_col)  \n",
    "            conv1 = maxpool2d(conv1, 2,1)   \n",
    "        else:\n",
    "            conv1 = conv2d(x, wc1, bc1,stride_row,stride_col)  \n",
    "            conv1 = maxpool2d(conv1, 2,1)   \n",
    "        variable_summaries(wc1)\n",
    "        variable_summaries(bc1)  \n",
    "    with tf.variable_scope('eigen_conv2'): \n",
    "        wc2= tf.get_variable( 'weight2',shape = [2, 1, dep1, dep2],\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)) \n",
    "        bc2=tf.get_variable( 'bias2',\n",
    "          shape = [dep2],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        stride_row=1;stride_col=1\n",
    "        if batch_norm:\n",
    "            conv2 = conv2d_norm(conv1,wc2,bc2, phase_train,stride_row,stride_col ) \n",
    "            conv2 = maxpool2d(conv2, 1,1)    \n",
    "        else:\n",
    "            conv2 = conv2d(conv1, wc2, bc2, stride_row,stride_col) \n",
    "            conv2 = maxpool2d(conv2, 1,1)   \n",
    "        variable_summaries(wc2)\n",
    "        variable_summaries(bc2)  \n",
    "    with tf.variable_scope('Singular_conv1'): \n",
    "        we1= tf.get_variable( 'wE1',shape = [2,1, 1, dep1],\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)) \n",
    "        be1=tf.get_variable( 'bE1',\n",
    "          shape = [dep1],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        stride_row=1;stride_col=1\n",
    "        if batch_norm:\n",
    "            conve1 = conv2d_norm(Energy,we1,be1, phase_train,stride_row,stride_col) \n",
    "            conve1 = maxpool2d(conve1, 2,1)  \n",
    "        else:\n",
    "            conve1 = conv2d(Energy, we1, be1,stride_row,stride_col) \n",
    "            conve1 = maxpool2d(conve1, 2,1)  \n",
    "        variable_summaries(we1)\n",
    "        variable_summaries(be1)\n",
    "    with tf.variable_scope('Singular_conv2'): \n",
    "        we2= tf.get_variable( 'wE2',shape = [2, 1, dep1, dep2],\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)) \n",
    "        be2=tf.get_variable( 'bE2',\n",
    "          shape = [dep2],\n",
    "          initializer=tf.constant_initializer(0.0))\n",
    "        stride_row=1;stride_col=1\n",
    "        if batch_norm:\n",
    "            conve2 = conv2d_norm(conve1,we2,be2, phase_train,stride_row,stride_col)\n",
    "            conve2 = maxpool2d(conve2, 1,1)  \n",
    "        else:\n",
    "            conve2 = conv2d(conve1, we2, be2,stride_row,stride_col)\n",
    "            conve2 = maxpool2d(conve2, 1,1)  \n",
    "        variable_summaries(we2)\n",
    "        variable_summaries(be2) \n",
    "    with tf.variable_scope('eigen_out'):\n",
    "        wfc = tf.get_variable('wfc',shape=[3*8*3   ,2*dep2],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bfc=tf.get_variable( 'bfc',\n",
    "          shape = [2*dep2],\n",
    "          initializer=tf.constant_initializer(0.0)), \n",
    "        wfce = tf.get_variable('wfce',shape=[8*2*3 ,2*dep2],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bfce=tf.get_variable( 'bfce',\n",
    "          shape = [2*dep2],\n",
    "          initializer=tf.constant_initializer(0.0)),  \n",
    "        wfc2 = tf.get_variable('wfc2',shape=[4*dep2  ,2*dep2],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bfc2=tf.get_variable( 'bfc2',\n",
    "          shape = [2*dep2],\n",
    "          initializer=tf.constant_initializer(0.0)), \n",
    "        # fully connected layer \n",
    "        fc0 = tf.reshape(conv2, [-1,  int(prod(conv2.get_shape()[1:])) ]) \n",
    "        fc1= tf.nn.relu(tf.add(tf.matmul(fc0, wfc), bfc))  \n",
    "        fce0 = tf.reshape(conve2, [-1,  int(prod(conve2.get_shape()[1:])) ]) \n",
    "        fce1= tf.nn.relu(tf.add(tf.matmul(fce0, wfce), bfce)) \n",
    "        fc2 = tf.concat([fc1,fce1],1) \n",
    "        fc2=tf.reshape(fc2, [-1, int(prod(fc2.get_shape()[1:]))])   \n",
    "        variable_summaries(wfc)\n",
    "        variable_summaries(bfc) \n",
    "        variable_summaries(wfce)\n",
    "        variable_summaries(bfce) \n",
    "    \n",
    "    with tf.variable_scope('Final_out'):\n",
    "        wout= tf.get_variable('wout',shape=[4*dep2, n_classes],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bout=tf.get_variable( 'bout',\n",
    "          shape = [n_classes],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        # fully connected layer \n",
    "        fc2=tf.cond( phase_train, lambda: fc2,lambda:tf.nn.dropout(fc2,keep_prob=keep_prob if dropout else 1.0)) \n",
    "        fc4= (tf.add(tf.matmul(fc2, wout), bout)) \n",
    "        \n",
    "    return fc4\n",
    "\n",
    "def establish_model(): \n",
    "    global keep_prob,   learning_rate, training_iters,display_step,batch_size,layers,patience,batch_norm , savepath\n",
    "    global train_data,train_e,train_labels,eval_data,eval_e,eval_labels,testX_true,testY_true , Energy_true,testX_pred,testY_pred , Energy_pred,testX_no,testY_no , Energy_no\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, dimx,layers,1])\n",
    "    y = tf.placeholder(tf.int32, [None, n_classes]) \n",
    "    e = tf.placeholder(tf.float32,[None, dimx,s_layers,1])\n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "        # Construct model\n",
    "    global_step = tf.Variable(0, trainable=False)  \n",
    "    pred = conv_net(  x, e,batch_norm,phase_train) \n",
    "    prob = tf.nn.softmax(pred)\n",
    "    predict_op=tf.argmax(prob, 1) \n",
    "    step=1\n",
    "    loss_list=[]\n",
    "    train_rate=[]\n",
    "    eval_rate=[]\n",
    "    n_incr_num =0\n",
    "    best_loss = np.Inf \n",
    "    # Define loss and optimizer \n",
    "    with tf.name_scope('loss'): \n",
    "        l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() )  \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))+l2  \n",
    "    with tf.name_scope('Optimizer'):  \n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate , decay=decay_c).minimize(cost, global_step=global_step)\n",
    "    with tf.name_scope('err'): \n",
    "        correct = tf.equal(predict_op, tf.argmax(y, 1))\n",
    "        err=1- tf.reduce_mean(tf.cast(correct, tf.float32))  \n",
    "    tf.summary.scalar('err',err)\n",
    "    tf.summary.scalar('loss',cost)\n",
    "    \n",
    "    #save\n",
    "    saver = tf.train.Saver()\n",
    "    # Launch the graph\n",
    "    sess = tf.InteractiveSession() \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    while step < training_iters:\n",
    "        ind = np.arange(train_data.shape[0])\n",
    "        batch_idx = np.random.choice(ind, batch_size, replace=False)\n",
    "        batch_x = train_data[batch_idx] \n",
    "        batch_y= train_labels[batch_idx]\n",
    "        batch_e = train_e[batch_idx]\n",
    "        indeval = np.arange(eval_data.shape[0]) \n",
    "        eval_idx = np.random.choice(indeval, batch_size, replace=False)\n",
    "        batch_xeval=eval_data[eval_idx]\n",
    "        batch_eval=eval_e[eval_idx]\n",
    "        batch_yeval=eval_labels[eval_idx]  \n",
    "        sess.run(optimizer, feed_dict={x: batch_x, e:batch_e,y: batch_y,phase_train:True }) \n",
    "        loss, train_err = sess.run([cost, err], feed_dict={x: batch_x, e:batch_e,y: batch_y ,phase_train:True})\n",
    "        loss_eval,eval_err=sess.run([cost ,err ], feed_dict={x: batch_xeval, e:batch_eval,y: batch_yeval ,phase_train:True})\n",
    "        loss_list.append(loss)\n",
    "        train_rate.append(train_err)\n",
    "        eval_rate.append(eval_err)\n",
    "        step += 1\n",
    "        if step % display_step == 0: \n",
    "            print(\"Iter \" + str(step ) + \", Minibatch Loss= \" +  \"{:.2f}\".format(loss) + \",training err= \" + \"{:.2f}\".format(train_err)+ \",validating err= \" + \"{:.2f}\".format(eval_err))   \n",
    "            if loss_eval < best_loss:\n",
    "                best_loss = loss_eval\n",
    "                n_incr_num =0\n",
    "            else:\n",
    "                n_incr_num+=1\n",
    "            if (n_incr_num >= patience) and (step > 7000):\n",
    "                print ('Early_stopping! and the iterations is', step) \n",
    "                tf.get_collection(\"validation_nodes\") \n",
    "                tf.add_to_collection(\"validation_nodes\", x) \n",
    "                tf.add_to_collection(\"validation_nodes\", y) \n",
    "                tf.add_to_collection(\"validation_nodes\", e) \n",
    "                tf.add_to_collection(\"validation_nodes\", phase_train) \n",
    "                tf.add_to_collection(\"validation_nodes\", prob) \n",
    "                results = predict_op\n",
    "                save_path = saver.save(sess, savepath+model_name) \n",
    "                correct_results_pred, results_pred=sess.run([correct, predict_op] , feed_dict={x:testX_pred, e:Energy_pred, y:testY_pred,phase_train:True}) \n",
    "                total_test_pred=sess.run(err , feed_dict={x:testX_pred, e:Energy_pred, y:testY_pred,phase_train:True}) \n",
    "                correct_results_no=sess.run(correct , feed_dict={x:testX_no, e:Energy_no, y:testY_no,phase_train:True}) \n",
    "                total_test_no=sess.run(err , feed_dict={x:testX_no, e:Energy_no, y:testY_no,phase_train:True})   \n",
    "                print(\"Testing err of subtract predicted :\", total_test_pred) \n",
    "                print(\"Testing err of subtract nothing :\", total_test_no)  \n",
    "                return loss_list,step,train_rate,eval_rate,  correct_results_pred,correct_results_no, results_pred  \n",
    "    tf.get_collection(\"validation_nodes\") \n",
    "    tf.add_to_collection(\"validation_nodes\", x) \n",
    "    tf.add_to_collection(\"validation_nodes\", y) \n",
    "    tf.add_to_collection(\"validation_nodes\", e) \n",
    "    tf.add_to_collection(\"validation_nodes\", phase_train) \n",
    "    tf.add_to_collection(\"validation_nodes\", prob)\n",
    "    save_path = saver.save(sess, savepath +model_name) \n",
    "    correct_results_pred, results_pred=sess.run([correct, predict_op] ,  feed_dict={x:testX_pred, e:Energy_pred, y:testY_pred,phase_train:True}) \n",
    "    total_test_pred=sess.run(err , feed_dict={x:testX_pred, e:Energy_pred, y:testY_pred,phase_train:True}) \n",
    "    correct_results_no=sess.run(correct , feed_dict={x:testX_no, e:Energy_no, y:testY_no,phase_train:True}) \n",
    "    total_test_no=sess.run(err , feed_dict={x:testX_no, e:Energy_no, y:testY_no,phase_train:True})   \n",
    "    print(\"Testing err of subtract predicted :\", total_test_pred) \n",
    "    print(\"Testing err of subtract nothing :\", total_test_no)  \n",
    "    print(\"Optimization Finished!\") \n",
    "    return loss_list,step,train_rate,eval_rate,  correct_results_pred,correct_results_no, results_pred   \n",
    "\n",
    "def each_perform(correct_results,eval_labels ):\n",
    "    label_y=np.argmax(eval_labels,1)\n",
    "    \n",
    "    zero = np.where(label_y==0)\n",
    "    correct=[correct_results[i] for i in zero]\n",
    "    accuracy_zero=1-np.mean(correct)\n",
    "\n",
    "    one = np.where(label_y==1)\n",
    "    correct=[correct_results[i] for i in one]\n",
    "    accuracy_one=1-np.mean(correct)\n",
    "\n",
    "    two = np.where(label_y==2)\n",
    "    correct=[correct_results[i] for i in two]\n",
    "    accuracy_two=1-np.mean(correct) \n",
    "    \n",
    "   \n",
    "    print(100-100*accuracy_zero)\n",
    "    print(100-100*accuracy_one)\n",
    "    print(100-100*accuracy_two)  \n",
    "\n",
    "def plot_loss(loss,train_step,from_second,name_save, plot_name,plot_title):\n",
    "    if from_second :\n",
    "        plt.plot(range(0,train_step-1,1),loss[1:])\n",
    "    else:\n",
    "        plt.plot(range(0,train_step,1),loss[0:])\n",
    "    plt.xlabel('Iterative times (t)')\n",
    "    plt.ylabel(plot_name)\n",
    "    plt.title(plot_title)\n",
    "    plt.grid(True) \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 1\n",
    "if train==1: \n",
    "    start_time = time.time()\n",
    "    loss ,step,train_rate,eval_rate,   correct_results_pred,correct_results_no, results_pred =establish_model()    \n",
    "    eclapse = time.time() -start_time\n",
    "    print ('Running time is',eclapse)\n",
    "    plot_loss(loss,step-1,False,\"Loss_value.png\", 'Loss','Loss function value with iterations') \n",
    "    plot_loss(train_rate,step-1,False,\"Train_err_rate.png\",'Training accurate rate','Training classification with iterations')\n",
    "    plot_loss(eval_rate,step-1,False,\"Eval_err_rate.png\",'Evaluating accurate rate','Evaluating classification with iterations')  \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
